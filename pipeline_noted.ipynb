{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this document\n",
    "\n",
    "The purpose of this noted version of pipeline is to guide the user through each step of the codes so that they are able to understand them as much as possible and equipped with knowledge to customize the script regardless of programming skills. It is **not** supposed to be runned as production tool. If you click \"Run All\" on this document, you **will** encounter errors. For productive purpose, consider modifying the accompanying **pipeline.ipynb** to suit your need.\n",
    "\n",
    "Before we start, it's highly recommended that you get familiar with basic python concepts and operations like [string manipulation](https://docs.python.org/3.4/library/string.html), [tuples, lists and dictionaries](https://docs.python.org/3/tutorial/datastructures.html), as well as a little bit about [object-oriented programming](https://python.swaroopch.com/oop.html) and [python modules](https://docs.python.org/3/tutorial/modules.html).\n",
    "\n",
    "Another note on the styling of this document: most of the sentences should (hopefully) make sense if taken literally. However, I try to use some syntax highlighting on the texts consistently to demonstrate the close relationship between the codes and thought process, as well as encouraging the reader to understand the already natural and self-explantory Python syntax and jargons. Specifically:\n",
    "\n",
    "-  a [hyperlink](https://en.wikipedia.org/wiki/Hyperlink) usually point to a well-defined python module or class or methods, especially when that concept is first encountered in this document and the reader would likely need some background reference. The link usually points to the official documentation of that concept, which might not be the best place to start for beginner. If you find the documentation puzzling, try to google that concept and find a tutorial that best suit your preference.\n",
    "-  an inline `code` usually refer to a name that already exsist in the [namespace](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) (i.e. the context where we run the codes in this document). It can be a previously encountered concepts, but more often it referes to variable or method names that we [imported](https://docs.python.org/3/reference/import.html) or defined along the way.\n",
    "-  **bold** texts are used more loosely to highlight anything that requires more attention than plain texts. Though it's not used as carefully as previous formats, it often refer to some specific values that a variable or method arguments can assume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells under this section should be executed every time the kernel is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set module paths and data path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the following cell would be the only thing you have to change when analyzing different datasets. `minian_path` should be the path that contains a folder named **\"minian\"** , under which the actual codes of **minian** (.py files) resides. The default value **\\.** means \"current folder\", which should work in most cases unless you want to try out another specific version of minian that is not in the same folder as this notebook. `dpath` is the folder that contains the actual videos, which are usually named **\"msCam\\*.avi\"** where \\* is a number. `meta_dict` is a python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that is used to construct meta data for the final labeled data structure. Currently we assume data are stored in some structured folders and we use folder names to fill in information. The default value means: the name of the last folder in `dpath` (the one that directly contains the videos) would be used as the value of a field named **'session_id'**, the name of the second-last folder in `dpath` would be the value for **'session'** and so on. Both the `keys` (field names) and `values` (numbers indicating which level of folder name should be used) of `meta_dict` can be modified to suit the specific user case. `chunks` is used to divide data into smaller chunks for parallel processing. Currently it is rarelly used and has little impact on the actual performance of the script. It is recommended to leave `chunks` to default. `in_memory` specify whether to load the whole dataset into memory. Since out-of-core computation are still not fully supported, it is recommended to leave `in_memory` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "minian_path = \".\"\n",
    "dpath = \"./demo_movies/\"\n",
    "meta_dict={'session_id': -1, 'session': -2, 'animal': -3}\n",
    "chunks = {'frame': 1000, 'height': 200, 'width': 200, 'unit_id':20}\n",
    "in_memory = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the necessary modules and usually should not be modified. If you encounter an `ImportError`, check that you followed the installation instructions and that `minian_path` is pointing to the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(minian_path)\n",
    "import gc\n",
    "import psutil\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import paramnb\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bpl\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import dask\n",
    "import datashader as ds\n",
    "from holoviews.operation.datashader import datashade, regrid, dynspread\n",
    "from datashader.colors import Sets1to3\n",
    "from dask.diagnostics import ProgressBar\n",
    "from IPython.core.display import display, HTML\n",
    "from dask.distributed import Client, progress\n",
    "from minian.utilities import load_videos, varray_to_tif, save_cnmf, save_movies, scale_varr, save_variable\n",
    "from minian.preprocessing import remove_brightspot, gradient_norm, denoise, remove_background, stripe_correction\n",
    "from minian.motion_correction import estimate_shift_fft, apply_shifts, interpolate_frame, mask_shifts\n",
    "from minian.initialization import seeds_init, gmm_refine, pnr_refine, intensity_refine, ks_refine, seeds_merge, initialize\n",
    "from minian.cnmf import get_noise_fft, get_noise_welch, update_spatial, update_temporal, unit_merge\n",
    "from minian.visualization import VArrayViewer, MCViewer, CNMFViewer, generate_videos, visualize_temporal_update, normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize `dpath` and initialize package for visualization. Re-run this cell if you are having trouble getting the visualization to show up on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dpath = os.path.abspath(dpath)\n",
    "hv.notebook_extension('bokeh', width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading videos and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of `load_videos` should be the path that contains the videos, which could be `dpath` we already defined. The second argument `pattern` is optional and is the [regular expression](https://docs.python.org/3/library/re.html) used to filter files under the specified folder. The default value **'msCam[0-9]+\\.avi$'** means that a file can only be loaded if its filename contain **'msCam'** then followed by at least one digit of number then **'.avi'** as the end of the filename. This can be changed to suit the naming convention of your videos. `in_memory` specify whether the whole video should be loaded into memory. Currently it is recommended to set `in_memory` to `True`. `dtype` is the underlying [data types](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.types.html) of the data. Usually `float32` is good enough and should be preferred to save memory demand. The resulting \"video array\" `varr` contains three dimensions: `height`, `width`, and `frame`. If you wish to downsample the video, pass in a `dictionary` to `resample`, whose keys should be the name of dimensions and values an integer specifying how many times that dimension should be reduced. For example, `resample={'frame': 2}` will temporally downsample the video with a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "varr = load_videos(dpath, in_memory=in_memory, dtype=np.float32, resample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell load videos and concatenate them together into `varr`, which is a [xarray.DataArray](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray). Now it's a perfect time to get familiar with this data structure and the [xarray](https://xarray.pydata.org/en/stable/) module in general, since we will be using these data structures throughout analysis. Basicly a `xarray.DataArray` is a labeled N-dimensional array. We can ask the computer to print out some information of `varr` by calling its name (as with any other variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that `varr` is a `xarray.DataArray` with a [name](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.name.html#xarray.DataArray.name) `'demo_movies'` and three dimensions: `frame`, `height` and `width`, each dimension is simply labeled with ascending natural numbers. The [dtype](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.dtype.html#xarray.DataArray.dtype) ([data type](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.types.html)) of `varr` is `numpy.float32`\n",
    "\n",
    "In addition to these information, we can visualize `varr` with the help of `VArrayViewer`, which shows the array as movie and also plot max, mean and minimum fluorescence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "vaviewer = VArrayViewer([varr], framerate=5)\n",
    "display(vaviewer.widgets)\n",
    "vaviewer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset part of video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to pre-processing, it's good practice to check if there is something obviously wrong with the video (like camera suddenly drops dark). This can usually be observed by visualizing the video and check the mean fluorescence plot. To take out some bad `frame`, let's say, `frame` after 800, we can utilize the [xarray.DataArray.sel](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.sel.html) method and [slice](https://docs.python.org/3/library/functions.html#slice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr.sel(frame=slice(None, 800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will subset `varr` along the `frame` dimension from the begining to the `frame` labeled **800**, and assign the result back to `varr_ref`, which is equivalent to taking out `frame` from **801** to the end. Note you can do the same thing to other dimensions like `height` and `width` to take out certain pixels of your video for all the `frame`s. For more information on using `xarray.DataArray.sel` as well as other indexing strategies, see [xarray documentation](http://xarray.pydata.org/en/stable/indexing.html)\n",
    "\n",
    "If your `varr` is fine, just assign it to `varr_ref` to keep the naming consitent with later codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we chunk `varr` into smaller pieces to use parallelization to accelerate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr_ref.chunk(dict(height=-1, width=-1, frame='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bright spots removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_brightspot` tries to correct for brightspots that are probably caused by dust on the imaging sensor. Usually this type of artifacts occupies one pixel which is significantly brighter than its surrounding pixels. Thus, for any given pixel, `remove_brightspot` calculate a difference of intensity between that pixel and the mean of the **four** surrounding pixels (that is, four pixels that are connected to the pixel in question by edge). Then for each frame, this difference value is taken for all pixels, and a zscore of them is calculated. The pixels whose zscore of difference that exceed `thres` is treated as \"brightspots\", and whose value will be substituted by the mean of its four surrounding pixels. Thus `thres` controls how strict brightspot detection would be. In stead of passing in numbers, you can also pass a string `'min'`, which means use the inverse of the minimum (most negative) value of zscore as the threshold. If you believe your videos are free of this type of artifacts, feel free to skip this step to reduce artificial assumptions and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following few cells during pre-processing, the functions (such as `remove_brightspot`) are evaluated lazily, which means it only creates a \"plan\" for the comuptation without doing it. Actual computations are carried out when we explicitly call the [persist](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.persist.html) method of the resulting `DataArray` (like `varr_ref.persist()`). This opens up possibility for [out-of-core computations](https://en.wikipedia.org/wiki/External_memory_algorithm), but it's currently not tested yet. So for now it is recommended to leave everything under the `if in_memory:` clause intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = remove_brightspot(varr_ref, thres=2)\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stripe correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier versions of miniscopes suffer from artifacts that looks like horizontal or vertical stripes across the image that's stationary across frames. `stripe_correction` tries to correct for this type of artifacts. Technically it calculate the mean along `'frame'` dimension (a \"mean frame\" if you will), then again calculate a mean along `reduce_dim` to get a one-dimensional representation of the \"stripes\", and then every frame in the movie is subtracted by this 1d stripe uniformly. Thus, `reduce_dim` should be the dimension (direction) the stripes run along. If you believe your videos are free from the stripes artifacts, feel free to skip this step to reduce artificial assumptions and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = stripe_correction(varr_ref, reduce_dim='height')\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale_varr` is a convenient function that act on `DataArray`s to linearly rescale every value to the range of 0 to 1 inclusive. If you don't like it, pass in `scale` to change the range or skip it altogether. Note that `scale` takes a `tuple` of 2 elements representing lower and upper bound of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = scale_varr(varr_ref, scale=(0, 1))\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save processed movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_variable` function takes in three required arguments: `var` is the variable you want to save, `fpath` is the path under which the actual data file will be stored, and `fname` is the file name of the data file. `meta_dict` is optional and will be used to fill in metadatas.\n",
    "\n",
    "In particular, here we are saving our minimally-processed videos in `DataArray` format. We give it a \"name\" `\"org\"` by calling the [rename](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rename.html) method on the array, which is `xarray`'s internal naming system that stays with the actual data and will be displayed when you print out the `DataArray` (In contrast, you can bind arbitrary variable name to the data like `whatever = varr_ref` without touching the data, which is less reliable and not accessible to other functions, which is probably why we need another name). In practice, give it a name that's human-readable and be sure to not name two pieces of data with the same name (Otherwise an error will occur if you try to combine them in a single dataset). In any case, the saved original videos are usually not used in the later part of the pipeline, except for the final `generate_videos` method which generate a movie to visualize the result of the pipeline. Thus if you are sure that you don't care about that movie, feel free to skip this to save both time and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_variable(var=varr_ref.rename(\"org\"), fpath=dpath, fname='minian', meta_dict=meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anisotropic diffusion in the next step requires a parameter `kappa`, which is best estimated with the [gradient](https://en.wikipedia.org/wiki/Image_gradient) of the image. See the description of the next step for details about this parameter and the whole process. Technically, here we choose the 90-th percentile `varr_gradient.quantile(0.9)` of the gradient of the first frame `varr_ref.isel(frame=0)` as kappa. If you believe this is a bad choice, or not want to use anisotropic diffusion at all, feel free to change or skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ProgressBar():\n",
    "    varr_gradient = gradient_norm(varr_ref.isel(frame=0)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = varr_gradient.quantile(0.9).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step carry out denoising of the image frame by frame. The method `denoise` takes in two required arguments: the first is the video array `varr`, and the second `method` is a string specifying the denoising method we are gonna use. Right now two methods are supported: `'gaussian'` and `'anisotropic'`. Under the hood, `denoise` simply call another function frame by frame in a parallel fashion. For `method='gaussian'` it calls [GaussianBlur](https://www.docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) from `OpenCV` package. For `method='anisotropic'` it calls [anisotropic_diffusion](http://loli.github.io/medpy/generated/medpy.filter.smoothing.anisotropic_diffusion.html) from `medpy` package. All additional [keyword arguments](https://docs.python.org/3.7/tutorial/controlflow.html#keyword-arguments) passed into `denoise` are directly passed into one of those two denoising functions under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) is straight-forward. All you need to do is pass in either the size of the gaussian kernel `ksize`, or the standard deviation of the kernel `sigmaX` and `sigmaY`. Pragmatically, `ksize=(3, 3)` with `sigmaX=0` (meaning adjust the standard deviation according to kernel size) works fairly well. For full controll of the process, refer to the [documentation](https://www.docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = denoise(varr=varr_ref, method='gaussian', ksize=(3, 3), sigmaX=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [anisotropic diffusion](https://en.wikipedia.org/wiki/Anisotropic_diffusion) is inspired by [MIN1PIPE](https://github.com/JinghaoLu/MIN1PIPE) package. Refer to [the paper](https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30826-X) for full detail. In a nutshell, gaussian blur can be think of as a diffusion process - that is, imagine your grey-scale image represent the concentration of some molecule, say saline, in a two dimesional dish, letting the saline diffuse for some time would be equivalent to carrying out a gaussian blur on the original image/concentration, and longer diffusion time correspond to gaussian blur with larger standard deviation. Anisotropic diffusion is a generalization of this idea - instead of diffusing uniformly, it make the diffusion slower where the gradient is higher. The result is a diffusion/denoising process that preserve the edges, and in theory is more favorable than gaussian blur. That said, `niter` is the number of iteration of this process, in other words, the \"time-step\" the diffusion is gonna last. The larger this number, the more blurred the image is gonna be. Pragmatically `niter=10` is good enought. Note that increasing this number will significantly increase the computation time since there more iterative steps for every frame. `kappa` decides who gets treated as \"edges\" and thus preserved. Pixels whose gradient is much larger than this number will diffuse much slower, whereas pixels with gradient lower than `kappa` will just flow. As mentioned before, pragmatically we found estimating `kappa` with the 90-th percentile of the gradient of an example frame works well. `gamma` control the speed of diffusion. In theory we want this as large as possible to save some `niter` and thus computation time. However it is advised to set this no larger than **0.25** otherwise weird things might happen. There were two equations that were proposed to control the diffusion process in the [original paper](https://dl.acm.org/citation.cfm?id=78304) of anisotropic diffusion, and `option` controls which one to use. Pragmatically `option=2` works better. For full control of the process, refer to the [documentation](http://loli.github.io/medpy/generated/medpy.filter.smoothing.anisotropic_diffusion.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = denoise(varr=varr_ref, method='anisotropic', niter=10, kappa=kappa, gamma=0.25, option=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, don't forget to actually run the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if in_memory:\n",
    "    with ProgressBar(), dask.config.set(scheduler='processes'):\n",
    "        Y = Y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step try to estimate backgrounds frame by frame and remove them. As with last step, the first argument to `remove_backgroun` is `varr` which is the video array to be processed, and the second is the `method` to use. Again there are two methods available: `'uniform'` or `'tophat'`. For both of them, they require a single parameter - a window size `wnd`, which is the third required argument to `remove_background`. The two methods differ in how background is estimated. For `method='uniform'`, a [uniform filter](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.ndimage.uniform_filter.html) (basically a two dimensional rolling mean) is applied to each frame. `wnd` control the window size of the filter, and the result is used as the background. The idea is that backgrounds mainly contains out-of-focus fluorescence, that is basicly a super-blurred version of the image itself. Thus a large (comparing to the expected size of a cell) number of `wnd` should work well. Pragmatically **50** works pretty good. For `method='tophat'`, a [disk element](http://scikit-image.org/docs/dev/api/skimage.morphology.html#disk) with a radius of `wnd` is created, then a [morphological erosion](https://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm) using the disk element is applied to each frame, which eats away any bright \"features\" that is smaller than the disk element. Then, a [morphological dilation](https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm) is applied to the \"eroded\" image, which in theory undo the erosion except the bright \"features\" that are completely eaten away will not be recovered. The overall effect of this process is to remove any bright feature that is smaller than a disk with radius `wnd`. Thus, when setting `wnd` to the expected size of **largest** cell, this process can give us a good estimation of the background. Pragmatically `wnd=10` works pretty well. The computation time of `method='tophat'` is significantly longer than `method='uniform'`, but the result is also visually much more appealing. Lastly, regardless of which `method` is chosen, `remove_background` will subtract the estimated background from the original movie frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = remove_background(varr=Y, method='tophat', wnd=10)\n",
    "if in_memory:\n",
    "    with ProgressBar(), dask.config.set(scheduler='processes'):\n",
    "        Y = Y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we rescale the video array to `range=(0, 1)`. Nothing is passed in here since we can just use the default of `scale_varr` (which, by the way, can be viewed with **Shift + Tab** when your text cursor is pointing to `scale_varr`, or any other function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = scale_varr(Y)\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        Y = Y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization of pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the result of pre-processing. `VArrayViewer` can be used to visualize videos side by side when passed in a [list](https://docs.python.org/3/tutorial/introduction.html#lists) of video arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=70\n",
    "vaviewer = VArrayViewer([varr_ref, Y.rename('Y')], framerate=5)\n",
    "display(vaviewer.widgets)\n",
    "vaviewer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare movie for motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use old method to generate movie for motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`method='tophat'` for background removal usually works pretty well at extracting only the neurons -- in fact it is a little bit too well for the purpose of motion correction, since we have to assume the fluorescence from the cells at adjacent frames are stationary otherwise there is no other landmarks we can use to perform motion correction. Sometimes this is a good enough assumption while other times this is not. In that case it is better to generate a video solely for the purpose of motion correction using `method='uniform'` for `remove_background`, which only remove the general blurred out-of-focus fluorescence, while preserving other details such as, most importantly, blood vessels. Note that we now have two copies of our videos: the minially processed original movie `varr_ref` and denoised, background-free `Y`. The cell below creates another one `varr_mc` for motion correction. This is very memory-demanding and you might easily run into `MemoryError`. A workaround (before we have full out-of-core computation support) is to dump `varr_ref` and `Y` into hard drive using `save_variable`, or directly [to_netcdf](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.to_netcdf.html) from `xarray`, then restart the kernel to clear out memories, load `varr_ref` using [open_dataarray](http://xarray.pydata.org/en/stable/generated/xarray.open_dataarray.html), get the `shifts` using the **estimate shifts** section below, and apply them to `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_mc = remove_background(varr_ref, 'uniform', 51)\n",
    "varr_mc = denoise(varr_mc, 'gaussian', ksize=(3, 3), sigmaX=0)\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        varr_mc = varr_mc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use Y for motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is, of course, simpy use `Y` for motion correction. Note the following line will not create a copy of `Y`, instead it simply bind the name `varr_mc` to `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_mc = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind `estimate_shift_fft` is simple: for each frame it calculates a two-dimensional [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) between that frame and a template using [fft](https://en.wikipedia.org/wiki/Fast_Fourier_transform). The argument `on` determine what we should use as template, it can be the first frame `on='first'`, the last frame `on='last'`, the mean frame `on='mean'`, or `on='perframe'`, meaning every frame will be registered to the immediate previous frame instead of a common template. After we have obtained the [cross-correlogram](https://en.wikipedia.org/wiki/Correlogram) for each frame, in theory we can simply look at the place/shift that has maximum correlation and use the inverse of that as `shifts`. However, due to some mysterious property of miniscope data, the maximum of the correlogram are usually biased towards weird places, most likely somewhere close to origin ((0, 0) shifts), and it's more reliable to use the [center_of_mass](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.ndimage.measurements.center_of_mass.html) of the correlogram rather than simply [argmax](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmax.html). There is yet another gotcha -- there are usually other local maximas that will bias `center_of_mass` and it's better to threshold those out before we take the `center_of_mass`. The argument `pct_thres` controls that -- it will set any value below that percentile to 0 in the cross-correlogram. Pragmatically `pct_thres=99.9` works well at taking out all the weird artifacts. The results from `estimate_shift_fft` are saved in a two dimensional `DataArray` called `res`, with three different `variable`s: the shifts along `'height'`, the shifts along `'width'`, and the actuall value of maximum correlation `corr`. And we have these three `variable`s across all `frame`s (hence the two dimensions). We can thus pull out `shifts` and `corr`s from `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = estimate_shift_fft(varr_mc, on='mean', pct_thres=99.9)\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        res = res.compute()\n",
    "shifts = res.sel(variable=['height', 'width'])\n",
    "corr = res.sel(variable='corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masking and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there are frames in the video that has a horrible shift and is simply way off the common field-of-view. For these frames it is not a good idea to shift them based on cross-correlation. Instead they should be treated as data loss and discarded/interpolated. `mask_shifts` tries to address this by simply look at the maximum correlations that is saved during `estimate_shift_fft`. The idea is if some frame has a way lower cross-correlation to template comparing to other frames, they are likely to be very bad frames. `mask_shifts` takes four required arguments: `varr_fft` which is the fft-transformed video arrays. `corr` which are the values of maximum correlations. `shifts` which are the shifts in both directions. All three of these comes from `estimate_shift_fft`. The fourth one `z_thres` gets to decide who is the \"bad\" frame -- the `corr`s will be z-scored, then any frame whose z-transformed `corr` is lower than `z_thres` would be treated as \"bad\" frames and interpolated. Another optional argument is `perframe`, it's a boolean value specifying whether `estimate_shift_fft` was carried out per-frame. If `perframe=True`, in addition to taking out \"bad\" frames, it will also recalculate the shifts of the two most adjacent \"good\" frames based on each other, that is, ignoring the \"bad\" frames who just got taken out. Obviously only set `perframe=True` if `on='perframe'` for `estimate_shift_fft`. The result of `mask_shifts` is an updated version of shifts `shifts_ma`, and a boolean `mask` specifying which are the \"bad\" frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shifts_ma, mask = mask_shifts(varr_fft, corr, shifts, z_thres=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `mask_shifts`, we use the returned `mask` to actually interpolate the bad frames. The interpolation are simply a mean frame across two most immediate adjacent \"good frames\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_mc = interpolate_frame(varr_mc.compute(), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determine shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take cumulative sum if `on='perframe'` when estimating shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `on='perframe'` for `estimate_shift_fft`, the shifts was calculated for each frame relative to the previous frame. Thus the real shift relative to the first frame should be a cumulative sum of `shifts`. Run the following cell only if you `on='perframe'` for `estimate_shift_fft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "shifts_final = shifts.cumsum('frame')\n",
    "shifts_final = np.around(shifts_final.fillna(0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use raw shifts otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `on is not 'perframe'` for `estimate_shift_fft`, simply run the following line to take a round of `shifts` and convert them to integer datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_final = np.around(shifts.fillna(0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply the `shifts_final` we got to the movie we want, which is `Y` (not necessarily `varr_mc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_mc = apply_shifts(Y, shifts_final)\n",
    "if in_memory:\n",
    "    with ProgressBar():\n",
    "        Y_mc = Y_mc.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization of shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize both the `shifts` and `shifts_final` as fluctuating curve along `frame`s. This is the first time we explicitly use the package [holoviews](http://holoviews.org) which is a really nice package for visualizing data, and I highly recommend reading through their tutorial to get familiar with the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "%%opts Curve [width=500, tools=['hover']]\n",
    "hv.NdOverlay(dict(width=hv.Curve(shifts.sel(variable='width')), height=hv.Curve(shifts.sel(variable='height'))))\\\n",
    "+ hv.NdOverlay(dict(width=hv.Curve(shifts_final.sel(variable='width')), height=hv.Curve(shifts_final.sel(variable='height'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization of motion-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we visualize the movies with `VArrayViewer`. The optional argument `framerate` only control how the frame slider behave, not how the data is handled. Note that `rename` here is necessary since `Y_mc` come from `Y` and share the same name. If we don't give them different names there is gonna be a `MergeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100 fps=5\n",
    "%%opts Image (cmap='Viridis')\n",
    "vaviewer = VArrayViewer([Y.rename('Y'), Y_mc.rename('Y_mc')], framerate=5)\n",
    "display(vaviewer.widgets)\n",
    "vaviewer.show().redim.range(Y=(0,0.2), Y_mc=(0, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save result as DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the pre-processed and motion-corrected data to hard drive as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_variable(Y_mc.rename('Y'), dpath, 'minian', meta_dict=meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this it is a perfect time to restart the kernel to free up some memory, since we will only work with `Y` from now on and do not need any of those itermediate huge videos anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole initialization section is adapted from the [MIN1PIPE](https://github.com/JinghaoLu/MIN1PIPE) package. See their [paper](https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30826-X) for full details about the theories. Here we only give minimal amount of information so that we can go through the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is open up the `minian.nc` dataset we have saved to hard drive. The [open_dataset](http://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html) function itself is lazy and should take less than a second. After that we want to explicitly load up `Y` into memory to speed things up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian = xr.open_dataset(os.path.join(dpath, 'minian.nc'), autoclose=True)\n",
    "Y = minian['Y'].load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to initialize the **seeds**. The idea is that we select some subset of frames, compute a max projection of those frames, then find the local maximums of that max projection. We keep repeating this process and putting together all the local maximums we get along the way, until we get an overly-complete set of local maximums/bright-spots, which are the potential location of cells and we call them **seeds**. The assumption here is that the center of cells have to be brighter than it's surroundings on some, but not necessarily all, frames. The first and only required argument `seeds_init` takes is `varr` -- the video array we want to process. There are four additional arguments controlling how we subset the frames: `wnd_size` controls the window size of each chunk, *i.e* the number of frames in each chunk; `method` can be either `'rolling'` or `'random'`. For `method='rolling'`, the moving window will roll along `frame`, whereas for `method='random'`, chunks with `wnd_size` number of frames will be randomly pick; `stp_size` is only used if `method='rolling'`, it is the step-size of the rolling windows, in other words the distance between the **center** of each rolling window. For example, if `wnd_size=100` and `stp_size=200`, the windows will be like: **(0, 50)**, **(150, 250)**, **(350, 450)** *etc.* (Obviously that was a bad choice since you probably want the windows to overlap otherwise you will miss cells); `nchunk` is only used if `method='random'`, It is the number of random chunks we will draw. The default values of `seeds_init` usually works fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds = seeds_init(Y, wnd_size=500, method='rolling', stp_size=200, nchunk=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now come the first step of cleaning the seeds -- by Gaussian Mixture Model. The idea is that for each seed we have, we calculate the difference between the peak and valley of the fluorecence signal corresponding to the seed pixel. If we put those peak-to-valley values for all the seeds together and look at the histogram of those values, they are supposed to be composed of two Gaussian distribution. We then fit a Gaussian Mixture Model of two components to those peak-to-valley values, and ask the model to predict for each value which Gaussian distribution it is likely to come from. We then keep only the seeds whose peak-to-valley value come from the Gaussian distribution with higher mean, that is, relatively large among all seeds. Thus the assumption here is that of all the \"bright\" seeds we have pulled out from the video, some of them barely changed their fluoresence across the whole recording session, while others changed a lot. Those who changes are cells whereas those who don't are weird random bright-spots. It can be speculated that if a cell was firing all the time during recording, they might be filtered out during this step. In that case you might want to skip this step. `gmm_refine` takes in the video array `varr`, the `seeds` to be refined, and an optional argument `q` controlling how we define \"peak\" and \"valley\". By default `q=(0.1, 99.9)` meaning we use **0.1** percentile as the \"valley\" and **99.9** percentile as the \"peak\". Usually it is fine to leave them as-is, but feel free to change them if you believe they are messing things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_gmm = gmm_refine(varr=Y, seeds=seeds, q=(0.1, 99.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pnr_refine` stands for \"peak-to-noise ratio\" refine. The \"peak\" and \"noise\" here are defined differently from before. First we seperate/filter the temporal signal for each seed based on frequency -- the signals composed of the lower half of the frequency are regarded as **real** signals, while the higher half of the frequency are presumably **noise** (\"half\" being relative to [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency)). Then we take the peak-to-valley value (really just **max** minus **min**, or, [np.ptp](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ptp.html)) for both the **real** signal and **noise** signal. Then finally, \"peak-to-noise ratio\" is simply the ratio between the `np.ptp` values of **real** and **noise** signal. So, the critical assumption here is that real activities are lower frequency while noise are higher frequency, and they seperate at approximately half the Nyquist frequency, or, one-fourth of the sampling frequency of the video, and we don't want those \"seeds\" whose **real** signals are buried in **noise**. If these assumptions does not suit your case, for example, if you have a really low sampling rate, or if your video are unavoidably noisy, consider skipping this step. As always, `pnr_refine` takes in `varr` and `seeds` as first two arguments, then a `thres`, where seeds whose \"peak-to-noise ratio\" fall below this threshold will be discarded. Pragmatically `thres=1.5` works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_pnr = pnr_refine(varr=Y, seeds=seeds_gmm, thres=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`intensity_refine` threshold the seeds based purely on the intensity, and here is how the threshold is found: We first calculate a max projection across all frames, literally `varr.max('frame')`. Then, we bin the max projection so that each bin has [around](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.around.html) 10 pixels. We then use those bins to generate a [histogram](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.histogram.html), and find the bin that has the highest frequency/density. Then the threshold is simply twice the intensity that correspond to the highest bin. `intensity_refine` only takes `varr` and `seeds` with no additional arguments. It can, however, potentially run into problems: In case you have a field-of-view that is bright overall, the threshold might simply be out of the range of the intensities. In that case `intensity_refine` simply do nothing (and it will tell you if that happens). Pragamatically that rarely happens and `intensity_refine` usually does not take away real cells. However if you found this step too arbitrary you should be able to safely skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_int = intensity_refine(varr=Y, seeds=seeds_pnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ks_refine` refine the seeds using [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test). The idea is simple: for each seeds, their fluorescence level on all frames should be somewhat [bimodal](https://en.wikipedia.org/wiki/Multimodal_distribution), with a large normal distribution representing silence/little activities, and maybe a tiny peak representing when the seed/cell is firing. Thus we can carry out KS test for all seeds against the normal distribution, and keep only the seeds where the null hypothesis (that the fluoresence is simply a normal distribution) is rejected and is defaulted to **0.05**. `ks_refine` takes in `varr` and `seeds` as first two arguments, then a `sig` which is the significance level at which the null hypothesis is rejected. Personally I found this step tend to take away real cells when the video is very short (for example, the one that comes with this package under `\"./demo_movies\"`), probably because the number of \"active\" frames are too small comparing to the majority of the frames. So feel free to skip this step if you encounter the same situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_ks = ks_refine(varr=Y, seeds=seeds_int, sig=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentially we could pick out multiple seeds that is actually within one cell, and we want to avoid that as much as possible. `seeds_merge` try to merge seeds together that potentially come from the same cell, based on spatial distance and temporal correlation. Specifically, `thres_dist` is the threshold for euclidean distance between pair of seeds, in pixels; `thres_corr` is the threshold for pearson correlation between pair of seeds. Any pair of seeds that are within `thres_dist` **and** has a correlation higher than `thres_corr` will be merged together, where \"merge\" meaning only the seed with maximum internsity in the max projection of the video will be kept. Thus `thres_dist` should be the expected size of the cells and `thres_corr` should be relatively high to avoid over-merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_mrg = seeds_merge(varr=Y, seeds=seeds_ks, thres_dist=5, thres_corr=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till now, the seeds we have are only one-pixel dots. In order to kick start CNMF we need something more like the spatial footprint `A` and temporal activities `C` of real cells. Thus we need to `initilalize` `A` and `C` from the `seeds_mrg` we have. Here is how it is done:For each seed, the temporal activities of surrounding pixels can be put into a matrix `sur` with dimension `frame` and `spatial`, where `spatial` is the flattened representation of pixel location, or, a [stack](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.stack.html)ed version of `height` and `width`. Let `sd` denote the temporal activity of the seed, we use the dot product of the temporal activities between the seed and the surrounding pixels `sur.dot(sd)` normalized by the [Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) of `sur`: [np.linalg.norm(sur)](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.norm.html), as well as `np.linalg.norm(sd)`, as the spatial footprint `a`. In the form of equation: $$\\mathbf{a} = \\frac{\\mathbf{sur} \\cdot \\mathbf{sd}}{\\Vert{\\mathbf{sur}}\\Vert \\Vert{\\mathbf{sd}}\\Vert}$$ We can think of this as a normalized measurement of how similar the temporal activities of each surrounding pixel is comparing to those of the seed. Then we use the dot product `sur.dot(a)` as the temporal trace for the seed, which is conceptually an \"averaged\" activities of all the pixels surrounding the seed weighted by `a`. After that we simply [concatenate](http://xarray.pydata.org/en/stable/generated/xarray.concat.html) the spatial footprints and temporal traces from different seeds together along a new dimension `unit_id` to get our initial `A` and `C` matrix. Finally we need two more terms: `b` and `f` representing the spatial footprint and temporal dynamic of the **background**, respectively. We find those by simply taking the dot product `A.dot(C)`, which is the activities from our initial units, and subtract that from the original movie `varr`. We use the mean of that remainder across `frame` as `b`, and the mean across `height` and `width` as `f`. `initilaize` takes in `varr` and `seeds` as the first two arguments. Two additional arguments determine how we find the `sur`roundings when calculating `a`. Firstly we certainly do not want to consider every other pixels in the movie as `sur` to a seed, and we just need a hard window where any pixels outside that window are ignored during the calculation of `a`. `wnd` is the half-size of that window. Secondly, not every pixels within the range of `wnd` are equally possible to be part of the cell represented by the seed. We thus put another correlation threshold -- pixels whose temporal activities has a pearson correlation with the seed higher than `thres_corr` will be considered as `sur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "A, C, b, f = initialize(varr=Y, seeds=seeds_mrg, thres_corr=0.8, wnd=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualize the result of our initialization by plotting a projection of the spatial matrix `A` as well as a raster of the temporal matrix `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(plot=dict(height=300, width=300))\n",
    "regrid(hv.Image(A.sum('unit_id'), kdims=['width', 'height'])).opts(**opts) + regrid(hv.Image(C, kdims=['frame', 'unit_id'])).opts(**opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the results in the dataset. Note here we change the name of a dimension by doing `rename(unit_id='unit_id_init')`, this is a precaution, since the dimension `unit_id` will likely to change after next section **CNMF** (most likely units will be merged), and there will be conflicts then if we save other variables with dimension `unit_id` that has different coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_variable(A.rename('A_init').rename(unit_id='unit_id_init'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(C.rename('C_init').rename(unit_id='unit_id_init'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(b.rename('b_init'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(f.rename('f_init'), dpath, 'minian', meta_dict=meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section assume you already have good knowledge about using CNMF as a method of extracting neural activities from the movie. If not, please read [the paper](https://www.sciencedirect.com/science/article/pii/S0896627315010843) before proceeding.\n",
    "\n",
    "As a quick reminder and check, here is the essential idea of CNMF: We believe our movie `Y`, with dimension `height`, `width` and `frame`, can be written in (and thus break down as) the following equation: $$\\mathbf{Y} = \\mathbf{A} \\cdot \\mathbf{C} + \\mathbf{b} \\cdot \\mathbf{f} + \\epsilon$$ where `A` is the spatial footprint of each unit, with dimension `height`, `width` and `unit_id`, and `C` is the temporal activities of each unit, with dimension `unit_id` and `frame`. `b` and `f` are the spatial footpring and temporal activities of some background, and $\\epsilon$ is the noise. Note that strictly speaking, matrix multiplication are usually only defined for two dimensional matrices, but our `A` here has three dimensions. So in fact we are taking the [tensor product](https://en.wikipedia.org/wiki/Tensor_product) of `A` and `C`, reducing the dimension `unit_id`. This might seem to complicate things (comparing to just treat `height` and `width` as one flattened `spatial` dimension), but I believe it help you understand the essence of why the equation make sense: when you take a dot product of any two \"matrices\" on certain **dimension**, all that is happening is a **product** followed by a **sum** -- you take the product for all pairs of matching numbers coming from the two \"matrices\", where \"match\" is defined by their index along the said dimension, and then you take the sum of all those products along the dimension. Thus when we take the tensor product of `A` and `C`, we are actually multiplying all those numbers in dimension `height`, `width` and `frame`, matched by `unit_id`, and then take the sum. Conceptually, for each unit, we are weighting the spatial footprint (`height` and `width`) by the fluorecense of that unit on given `frame`, which is the **product**, and then we are overlaying all units together, which is the **sum**. With that, the equation above is trying to say that our movie is made up of a weighted sum of the spatial footprint and temporal activities of all units, plus some background and noise.\n",
    "\n",
    "Now, there is another rule about `C` that separate it from background and noise, and save it from being just some random matrix that happen to fit well with the data `Y` without having any biological meaning, and that is the second essential idea of CNMF: each \"row\" of `C`, which is the temporal trace for each unit, should be described as an [autoregressive process](https://en.wikipedia.org/wiki/Autoregressive_model)(AR process), with a parameter `p` defining the **order** of the AR process: $$ c(t) = \\sum_{i=0}^{p}\\gamma_i c(t-i) + s(t) + \\epsilon$$ where $c(t)$ is the calcium concentration at time (`frame`) $t$, $s(t)$ is spike/firing rate at time $t$ (what we actually care), and $\\epsilon$ is again some noise. Basicly this equation is trying to say that at any given time $t$, the calcium concentration at that moment $c(t)$ depends on the spike at that moment $s(t)$, as well as its own history up to `p` time-step back $c(t-i)$, scaled by some parameters $\\gamma_i$s, plus some noise $\\epsilon$. Another intuition of this equation come from looking at different `p`s: when `p=0`, calcium concentration is an exact copy of the spiking activities, which is probably not true; when `p=1`, calcium concentration is an exponential decay with instant rise triggered by the spikes; when `p=2`, calcium concentration is an exponential decay that has some rising time; when `p>2`, more convoluted waveforms start to emerge. (all of these with some noise, of course)\n",
    "\n",
    "With that, all CNMF is doing is trying to find the `A` and `C` (along with `b` and `f`) that best describe `Y`. There are two more important practical concerns: Firstly we cannot solve this problem in one shot, we need to iteratively and separately update `A` and `C` to approach the true solution, and we need something to start with (that is what **initilization** section is about). Surprisingly often times 2 iterative steps seem to give us good enough results, but you can always add more iterations (and you should be ablt to easily do that after reading the comments); Secondly, by intuition you may define \"best describe `Y`\" as the results that minimize the noise $\\epsilon$ (or residule, if you will). However we have to control for the [sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) of our results as well, since we do not want every little random pixel that happen to correlate with a cell to be counted as part of the spatial footprint of the cell (non-sparse `A`), nor do we want a tiny spike at every frame trying to explain every noisy peak we observe (non-sparse `C`). Thus the balance between fidelity (minimizing error) and sparsity (minimizing non-zero entries) is an important idea for both the spatial and temporal update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in our data from previous steps. The `'unit_id'` renamming lines are just for the precaution metioned at the end of **initialization** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chk = chunks.copy()\n",
    "chk['unit_id_init'] = chk.pop('unit_id')\n",
    "minian = xr.open_dataset(os.path.join(dpath, 'minian.nc'), chunks=chk)\n",
    "Y = minian['Y']\n",
    "A_init = minian['A_init'].rename(unit_id_init='unit_id')\n",
    "C_init = minian['C_init'].rename(unit_id_init='unit_id')\n",
    "b_init = minian['b_init']\n",
    "f_init = minian['f_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate spatial noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can not just ask the computer to minimize the noise as much as possible, we have to get a sense of how much noise is expected before balancing sparsity and error during the minimizing process. `get_noise_fft` will calculate the noise power for all pixels using fft. Essentially we compute an fft-transform for every pixel indepedently, which left us with a [power spectral density](https://en.wikipedia.org/wiki/Spectral_density) with dimension `spatial` and `freq`, where `spatial` is flattened `height` and `width`, and `freq` is the dimension representing different frequency component of the signal. The power spectrum is the second returnned value of `get_noise_fft`, which is assigned to `psd` in the following code. We then need to define noise with the spectrum. By default `noise_range=(0.25, 0.5)`. Note that the number here is relative to the sampling frequency. So **0.5** actually represent Nyquist frequency and is the highest you can go as far as fft concerns. Thus **(0.25, 0.5)** is the higher frequency half of the signal, which is consistent with our assumption during **initialization** section. After choosing `noise_range`, we have to decide how to collapse across different `freq` to get a single number of noise power for each pixel. Three `noise_method`s are availabe: `noise_method='mean'` and `noise_method='median'` will use the mean and median across all `freq` as the estimation of noise for each pixel. `noise_method='logmexp'`is a bit more complicated -- the equation goes like: $sn = \\exp( \\operatorname{\\mathbb{E}}[\\log psd] )$ where $\\exp$ is the [exponential function](Exponential_function), $\\operatorname{\\mathbb{E}}$ is the [expectation operator](https://en.wikipedia.org/wiki/Expected_value) (mean), $\\log$ is [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm), $psd$ is the spectral density of noise for any pixel and $sn$ is the resulting estimation of noise power. It is recommended to keep `noise_method='logmexp'` since this is the default behavior of the [CaImAn](https://github.com/flatironinstitute/CaImAn) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sn_spatial, psd = get_noise_fft(varr=Y, noise_range=(0.25, 0.5), noise_method='logmexp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we plot the power spectrum `psd`. Note that the range of `psd` are usually wild and it is usually helpful to limit the range of our color with [`redim.range()`](http://holoviews.org/user_guide/Annotating_Data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [height=300, width=800, colorbar=True] (cmap='Viridis')\n",
    "psd_flt = psd.stack(spatial=['height', 'width'])\n",
    "hv_psd = hv.Image(psd_flt.assign_coords(spatial=range(psd_flt.sizes['spatial'])).rename('psd'), kdims=['spatial', 'freq'])\n",
    "regrid(hv_psd).redim.range(psd=(0, 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly select units for parameter exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will soon do some parameter exploring before actually commit to a spatial update, since you do not want to do a 10-minutes' worth step only to find out you started with some rediculous value for certain parameter. Obviously we only want to run the parameter exploring on very small subset of data. So here we randomly select 10 units from `A_init.coords['unit_id']` with the help of [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.random.choice(A_init.coords['unit_id'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test parameters for spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually do the parameter exploring. It seem to be the most complicated chunk of code so far, but it is very important to read through it, since we will see similar things later and it is a very powerful piece of code that can help you visualize a lot of stuff. The basic idea is we run `update_spatial` on a small subset (`units`) of data using different parameters within a `for` loop, and after that visualize the result using `holoviews`. We will go line by line: first two lines create `opts_A` and `opts_C`, which is the plotting options for plotting `A` and `C`. If you haven't got a chance to check out [the tutorial](http://holoviews.org/user_guide/Customizing_Plots.html) on plotting options from `holoviews`, it is time. Then we create a `list` of values we want to try out. Since we are interested in the parameter `sparse_penal`ty, we create a `sprs_ls`. We then create an empty `dict`ionary to store the resulting `A`s, thus we call it `A_dict`. Then there is the `for` loop iterating through `sprs_ls`, with one of the values from the list as `cur_sprs` during each iteration. Within the loop, we run `update_spatial`. The exact arguments `update_spatial` needs will be discussed later, but basicly we just have to feed it a bunch of data, including the movie `Y`, initial values of `A_init`, `b_init`, `C_init`, and `f_init`, the spatial noise we just estimated `sn_spatial`. Then we feed in the parameter we are playing with `sparse_penal=cur_sprs`. Note that we subset our data here by `sel`ecting the ten `units` we just randomly chose. Then we created [`hv.Image`](http://holoviews.org/reference/elements/bokeh/Image.html) from the resulting `cur_A` and `cur_C`. Again if you haven't check out [the tutorial](http://holoviews.org/getting_started/Introduction.html) on how to create basic elements, please do. After that we combine the two image by simply `+` them together, and ask them to ocuupy only one column `.cols(1)`, and store this whole layout into `A_dict`, with the keys being the `cur_sprs` corresponding to the result of this iteration. After all the loops are done, we generate a [`hv.HoloMap`](http://holoviews.org/reference/containers/bokeh/HoloMap.html) from `A_dict` so that we can visualize all the results in an animated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts_A = dict(plot=dict(height=480, width=752), style=dict(cmap='Viridis'))\n",
    "opts_C = dict(plot=dict(height=480, width=1000), style=dict(cmap='Viridis'))\n",
    "sprs_ls = [5e-6, 5e-3, 0.5, 5]\n",
    "A_dict = dict()\n",
    "for cur_sprs in sprs_ls:\n",
    "    cur_A, cur_b, cur_C, cur_f = update_spatial(\n",
    "        Y, A_init.sel(unit_id=units),\n",
    "        b_init, C_init.sel(unit_id=units), f_init, sn_spatial, sparse_penal=cur_sprs)\n",
    "    hv_cur_A = hv.Image(cur_A.sum('unit_id'), kdims=['width', 'height']).opts(**opts_A)\n",
    "    hv_cur_C = hv.Image(cur_C, kdims=['frame', 'unit_id']).opts(**opts_C)\n",
    "    A_dict[cur_sprs] = (hv_cur_A + hv_cur_C).cols(1)\n",
    "hv_res = hv.HoloMap(A_dict, kdims=['sparse_penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we actually plot out `hv_res`. What you should expect here will be explained later along with what `sparse_penal` actually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [colorbar=True] {+axiswise}\n",
    "hv_res.collate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first spatial update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial, b_spatial, C_spatial, f_spatial = update_spatial(\n",
    "    Y, A_init, b_init, C_init, f_init, sn_spatial, sparse_penal=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts Image [colorbar=True] (cmap='Viridis')\n",
    "regrid(hv.Image(A_init.sum('unit_id'), kdims=['width', 'height'])).opts(plot=dict(height=480, width=752))\\\n",
    "+ regrid(hv.Image(A_spatial.sum('unit_id').rename('A_spatial'), kdims=['width', 'height'])).opts(plot=dict(height=480, width=752)).redim.range(A_spatial=(0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test parameters for temporal update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import itertools as itt\n",
    "p_ls = [1]\n",
    "sprs_ls = [0.5, 5]\n",
    "add_ls = [5]\n",
    "noise_ls = [0.25]\n",
    "vis_dict = dict()\n",
    "for cur_sprs, cur_p, cur_add, cur_noise in itt.product(sprs_ls, p_ls, add_ls, noise_ls):\n",
    "    print(\"processing {}\".format((cur_p, cur_sprs, cur_add, cur_noise)))\n",
    "    YrA, cur_C, cur_S, cur_B, cur_C0, cur_sig, cur_g, = update_temporal(\n",
    "        Y, A_spatial.isel(unit_id=slice(10, 20)), b_spatial, C_spatial.isel(unit_id=slice(10, 20)),\n",
    "        f_spatial, sn_spatial, sparse_penal=cur_sprs, p=cur_p, use_spatial=False, use_smooth=True,\n",
    "        add_lag = cur_add, noise_freq=cur_noise, chk=dict(frame=200, unit_id=20),\n",
    "        cvx_sched=\"processes\")\n",
    "    # YrA = YrA.assign_coords(unit_id = range(YrA.sizes['unit_id']))\n",
    "    # cur_C = cur_C.assign_coords(unit_id = range(cur_C.sizes['unit_id']))\n",
    "    # cur_S = cur_S.assign_coords(unit_id = range(cur_S.sizes['unit_id']))\n",
    "    # cur_g = cur_g.assign_coords(unit_id = range(cur_g.sizes['unit_id']))\n",
    "    # cur_sig = cur_sig.assign_coords(unit_id = range(cur_sig.sizes['unit_id']))\n",
    "    vis_dict[(cur_p, cur_sprs, cur_add, cur_noise)] = visualize_temporal_update(\n",
    "        YrA, cur_C, cur_S, cur_g, cur_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=800] {+framewise}\n",
    "hv_res = hv.HoloMap(vis_dict, kdims=['p', 'sparse_penalty', 'add_lag', 'noise_freq']).collate()\n",
    "hv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first temporal update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal, S_temporal, B_temporal, C0_temporal, sig_temporal, g_temporal = update_temporal(\n",
    "        Y, A_spatial,\n",
    "        b_spatial, C_spatial, f_spatial, sn_spatial, jac_thres=0.2,\n",
    "        noise_freq=0.45, sparse_penal=0.5, p=1, add_lag = 0, use_spatial=False, chk=dict(frame=2000, unit_id=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts Curve [width=1200] {+framewise}\n",
    "visualize_temporal_update(YrA, C_temporal, S_temporal, g_temporal, sig_temporal, normalize=False).select(unit_id = slice(0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [colorbar=True] (cmap='Viridis')\n",
    "hv_c = regrid(hv.Image(C_temporal.rename('c'), kdims=['frame', 'unit_id'])).opts(plot=dict(height=500, width=1000)).redim.range(c=(0, 1))\n",
    "hv_s = regrid(hv.Image(S_temporal.rename('s'), kdims=['frame', 'unit_id'])).opts(plot=dict(height=500, width=1000)).redim.range(s=(0, 0.006))\n",
    "(hv_c + hv_s).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_mrg, sig_mrg = unit_merge(A_spatial, sig_temporal, thres_corr=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second spatial update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2 = update_spatial(\n",
    "    Y, A_mrg, b_spatial, sig_mrg, f_spatial, sn_spatial, sparse_penal=0.1, dl_wnd=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_spatial_it2_norm = xr.apply_ufunc(normalize, A_spatial_it2, input_core_dims=[['height', 'width']], output_core_dims=[['height', 'width']], vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid(hv.Image(A_spatial.sum('unit_id'), kdims=['width', 'height'])).opts(plot=dict(height=480, width=752))\\\n",
    "+ regrid(hv.Image(A_spatial_it2_norm.sum('unit_id'), kdims=['width', 'height'])).opts(plot=dict(height=480, width=752))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second temporal update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal_it2, S_temporal_it2, B_temporal_it2, C0_temporal_it2, sig_temporal_it2, g_temporal_it2 = update_temporal(\n",
    "    Y, A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2, sn_spatial, jac_thres=0.2,\n",
    "    noise_freq=0.45, sparse_penal=0.5, p=1, add_lag=0, chk=dict(frame=2000, unit_id=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts Curve [width=1200] {+framewise}\n",
    "visualize_temporal_update(\n",
    "    YrA, C_temporal_it2, S_temporal_it2, g_temporal_it2, sig_temporal_it2).select(unit_id=slice(0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [colorbar=True, tools=['hover']] (cmap='Viridis')\n",
    "hv_c = regrid(hv.Image(C_temporal_it2.rename('c'), kdims=['frame', 'unit_id'])).opts(plot=dict(height=500, width=1000)).redim.range(c=(0, 1))\n",
    "hv_s = regrid(hv.Image(S_temporal_it2.rename('s'), kdims=['frame', 'unit_id'])).opts(plot=dict(height=500, width=1000)).redim.range(s=(0, 0.006))\n",
    "(hv_c + hv_s).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian.close()\n",
    "save_variable(A_spatial_it2.rename('A'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(C_temporal_it2.rename('C'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(S_temporal_it2.rename('S'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(g_temporal_it2.rename('g'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(b_spatial_it2.rename('b'), dpath, 'minian', meta_dict=meta_dict)\n",
    "save_variable(f_spatial_it2.rename('f'), dpath, 'minian', meta_dict=meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minian = xr.open_dataset(os.path.join(dpath, 'minian.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generate_videos(minian, os.path.join(dpath, \"minian.mp4\"), chk=dict(height=100, width=100, frame=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnmfviewer = CNMFViewer(minian, minian['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnmfviewer.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (minian)",
   "language": "python",
   "name": "minian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "pipeline.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
